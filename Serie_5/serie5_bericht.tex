\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{xcolor}
\usepackage[nottoc]{tocbibind}
\usepackage{caption}

\colorlet{shadecolor}{gray!25}
\setlength{\parindent}{0pt}

\newcommand{\abs}[1]{\left\lvert#1\right\rvert}


\begin{document}

\title{Lösen des Poisson-Problems mittels Finite-Differenzen-Diskretisierung\\
und CG-Verfahren}
\author{Marisa Breßler und Anne Jeschke (PPI27)}
\date{07.02.2020}
\maketitle

\tableofcontents

\pagebreak
\section{Motivation}
In unserem vorherigen Berichten haben wir das Poisson-Problem vorgestellt und einen numerischen Lösungsansatz aufgezeigt, der es durch eine Diskretisierung des Gebietes und des Laplace-Operators in das Lösen eines linearen Gleichungssystems überführt.
Die dabei entstehende tridiagonale Blockmatrix ist dünn besetzt, d.h. nur wenige Einträge sind ungleich Null.
Deswegen ist es sinnvoll, sie als sogenannte \textit{sparse}-Matrix abzuspeichern.
Dieser Speicherplatzvorteil geht jedoch beim Lösen des linearen Gleichungssystems mittels Gauß-Algorithmus und LU-Zerlegung verloren, da eine LU-Zerlegung einer dünn besetzten Matrix im Allgemeinen nicht dünn besetzt ist.
Aufgrund dieses Umstandes erscheint es sinnvoll, das lineare Gleichungssystem nicht direkt, sondern iterativ zu lösen.
Solche iterativen Lösungsverfahren (wie das Gesamtschrittverfahren von Jacobi, das Einzelschrittverfahren von Gauß-Seidel, das SOR-Verfahren und das CG-Verfahren) bieten gegenüber direkten Verfahren außerdem den Vorteil, dass deren Rechenaufwand im Verhältnis zur in der Praxis für gewöhnlich sehr großen Dimension der Blockmatrix recht gering ist.
Da das CG-Verfahrens eine heute durchaus übliche Methode zum Lösen großer linearer Gleichungssysteme darstellt, wollen wir dieses im Folgenden im Rahmen unseres bisherigen Settings (Poisson-Problem und Finite-Differenzen-Diskretisierung) vorstellen und untersuchen.



\pagebreak
\section{Theoretische Grundlagen und Algorithmus}

Im Gegensatz zu den bisher behandelten direkten Verfahren, die nach einer endlichen Anzahl von Rechenschritten den Lösungsvektor $x$ des linearen Gleichungssystems $Ax=b$ mit $A\in\mathbb{R}^{N\times N}$ und $b\in\mathbb{R}^N$ liefern, definiert man bei den iterativen Verfahren eine Folge $(x_k)$ von Vektoren, die bei einem beliebigen Vektor startet und deren Grenzwert für $k \to \infty$ der genaue Lösungsvektor ist.
Man wählt dann ein $\epsilon\in\mathbb{R}$ und berechnet so viele Iterationsschritte, bis das Residuum, also der approximierte Abstand des in diesem Schritt berecheten Folgenelementes zu der exakten Lösung, kleiner als $\epsilon$ ist.

Das CG-Verfahren (kurz für \textit{conjugated gradient}) ist ein Verfahren, das nur für symmetrisch positiv definite Matrizen $A$ funktioniert.
Diese Voraussetzung wird von unserer Blockmatrix erfüllt, denn sie ist per Definition symmetrisch und irreduzibel diagonaldominant.

\subsection{Idee des CG-Verfahrens}

Während sich die anderen oben genannten iterativen Verfahren die Methode des Splitting zunutze machen, ist die Idee des CG-Verfahrens die Lösung des linearen Gleichungssystems in ein Minimierungsproblem zu überführen.

Dazu definiert man mit dem Standardskalarprodukt $<.,.>$ die Funktion
\begin{align*}
  \Phi (x) &\coloneqq \frac{1}{2}<x,Ax> - <b,x>\\
           &=\frac{1}{2}x^T Ax - x^T b
\end{align*}

Diese besitzt genau ein globales Minimum. Dieses liegt genau an der Stelle $x$, die der Lösung von $Ax=b$ entspricht. Man konstruiert also, um die Lösung von $Ax=b$zu finden, iterativ eine Folge von immer kleiner werdenden Funktionswerten von $\Phi$.\\

Um das globale Minimum zu finden, sucht man ausgehend von einem Startwert zunächst nach dem Minimum entlang einer Startrichtung (eines Startgradienten).
Von diesem Minimum aus wählt man eine neue Suchrichtung (einen neuen Gradienten), die zu allen vorherigen Suchrichtungen $A$-konjugiert, d.h. orthogonal in der $A$-Norm, ist und sucht erneut nach einem Minimum entlang dieses Gradienten.
Dies tut man so lange, bis das Residuum kleiner oder gleich $\epsilon$ ist.
Da es nur genau $N$ zueinander $A$-konjugierte Richtungen gibt, würde man mit dem CG-Verfahren in der Theorie nach maximal $N$ Schritten sogar die exakte Lösung finden.
In der Praxis benötigt man jedoch den Toleranzfaktor $\epsilon$, da durch Rundungsfehler die Suchrichtungen unter Umständen nicht mehr genau orthogonal zueinander stehen und diese Garantie somit verloren geht.\\

\subsection{Algorithmus}

\underline{Initialisierung}: Man startet bei einem beliebigen $x_0\in\mathbb{R}^N$ und wählt einen Toleranzfaktor $\epsilon$.
Man setzt $k\coloneqq 0$ und berechnet das Anfangsresiduum $r_0 \coloneqq Ax_0 - b$. Die erste Suchrichtung ist $d_0 = -r_0$.\\

\underline{Schritt k}: Falls $||r_k||\leq \epsilon$, hat man bereits eine Lösung im Toleranzbereich gefunden und kann hier aufhören.

Sonst berechnet man $c_k \coloneqq <r_k, r_k>/<Ad_k,d_k>$. Dies gibt an, wie weit man in die Suchrichtung gehen muss, um das neue Minimum zu finden.

Dann gilt $x_{k+1} = x_k + c_k d_k$ und $r_{k+1} = r_k + c_k Ad_k = Ax_{k+1} - b$.

Man setzt $\beta_k \coloneqq <g_{k+1},g_{k+1}>/<g_{k},g_{k}>$.

Die neue zu $d_k$ $A$-konjugierte Suchrichtung ist dann $d_{k+1}\coloneqq -r_{k+1} + \beta_k d_k$.

Nun setzt man $k \coloneqq k+1$ und geht erneut zu Schritt k.\\

So nähert man sich Schritt für Schritt einer genauen Lösung von $Ax=b$.
Um zu verhindern, dass der Algorithmus für sehr viele Iterationen läuft ohne, dass sich das Ergebnis signifikant verbessert, kann man zusätzliche Abbruchbedingungen definieren, wie z.B. eine minimale Reduktion des Residuums nach jedem Schritt oder eine maximale Anzahl der Iterationsschritte.

\cite{tischendorf2019}

\pagebreak
\section{Experimente und Beobachtungen}
Im Folgenden wollen wir verschiedene Experimente präsentieren. 
Da wir im Gegensatz zu unserer Arbeit vom 03.01.2020 im hiesigen Rahmen das Gleichungssystem nicht direkt mittels Gauß-Algorithmus und LU-Zerlegung, sondern iterativ mittels CG-Verfahren lösen, gilt unser Interesse der Vorstellung der neuen Methode zum Ermitteln einer Lösung des Poisson-Problems sowie der Gegenüberstellung der beiden Verfahrensweisen.  


\subsection{Entwicklung des absoluten Fehlers pro Iteration}
Während direkte Verfahren nach einer endlichen Anzahl an Rechenoperationen \glqq{die exakte Lösung}\grqq{} liefern (dabei handelt es sich in der Praxis nicht um die exakte analytische Lösung, da ein Computer aufgrund seiner begrenzten Rechenleistung im Allgemeinen immer bei der Verarbeitung von Zahlen Fehler produziert), ermitteln iterative Verfahren schrittweise eine immer bessere Approximation der gesuchten Lösung. \\ 
Diese sukzessive Annäherung an die exakte Lösung lässt sich an den folgenden drei Grafiken (Abb. 1) erkennen. 
Sie zeigen jeweils für den ein-, den zwei- und den dreidimensionalen Fall die Entwicklung des absoluten Fehlers pro Iteration. 
(Startwert ist der Nullvektor.) 
Der Graph schmiegt sich mit zunehmender Schrittzahl immer mehr der x-Achse an. 
Dieses asymptotische Fehlerverhalten ist charakteristisch für iterative Verfahren. \\

{
  \centering
    \includegraphics[width=0.45\textwidth]{Grafiken/iterates_d1_n10}
    \includegraphics[width=0.45\textwidth]{Grafiken/iterates_d2_n10}
    \includegraphics[width=0.45\textwidth]{Grafiken/iterates_d3_n10}
    \vspace{-0.2cm}
    \captionof{figure}{Entwicklung des absoluten Fehlers pro Iteration für $d\in\{1, 2, 3\}$ und $n=10$}
}
\vspace{0.5cm}

Darüber hinaus ist auffallend, dass weitaus weniger als $N=(n-1)^d$ Schritte vonnöten sind, um eine sehr gute Näherungslösung zu errechnen. 
Während wir im eindimensionalen Fall, $N=9$, bereits nach der fünften Iteration eine ausreichend genaue Lösung haben, benötigen wir im zweidimensionalen Fall, $N=81$, nur 13 und im dreidimensionalen Fall, $N=729$, lediglich eine mehr, nämlich 14 Iterationen.  
(Die große Konvergenzgeschwindigkeit lässt sich auf die gute Kondition der tridiagonalen Blockmatrix zurückführen, die wir im vorherigen Bericht ausführlich untersucht haben.) 
Doch bei allen ist der Fehler schon nach acht Schritten nur noch sehr gering. 
Dass unser Programm dennoch weiterrechnet, ist dem Umstand geschuldet, dass noch keiner der eingegebenen Abbruchparameter zum Tragen kam. 
Da die Rechenschritte bei direkten Verfahren wie gesagt im Voraus bekannt und begrenzt sind, bei iterativen jedoch nicht, benötigen letztere Abbruchbedingungen, die den Schleifendurchlauf des Algorithmus zum Ende bringen. 
Unsere Implementierung der Methode der konjugierten Gradienten (CG) enthält die folgenden drei Abbruchbedingungen. 
\glqq{eps}\grqq: Abbruch, falls eine ausreichend genaue Lösung gefunden wurde, d.h. falls die Norm des Residuums eine vorgegebene Grenze unterschreitet (das Residuum, das im Bildraum von der Matrix $A$ existiert, ist ein Anhaltspunkt für die Größe des Fehlers unserer Lösung für den gesuchten Vektor $x$), \glqq{max\_iter}\grqq: Abbruch, falls der vorgenommene Aufwand ausgeschöpft wurde, d.h. falls eine vorgegebene Anzahl an Iterationen erreicht ist, \glqq{min\_red}\grqq: Abbruch, falls das Verfahren \glqq{feststeckt}\grqq, d.h. falls der Abstand zwischen den letzten beiden Residuen eine vorgegebene Grenze überschreitet. 
Dabei ist zu bemerken, dass das Greifen einer der letzten beiden Abbruchbedingungen darauf hinweist, dass das Lösungsverfahren nicht so funktioniert, wie es soll. 
Daraufhin könnte man seine Eingabe eines Startwertes oder der Feinheit korrigieren oder u.U. auf eine ganz andere Methode zurückgreifen. 


\subsection{Konvergenzverhalten für ein festes Epsilon}
Um den Unterschied zwischen der direkten Methode mittels Gauß-Algorithmus mit LU-Zerlegung und dem CG-Verfahren zu untersuchen, erschien es uns sinnvoll, das Konvergenzverhalten beider gegenüberzustellen. 
Dieser Vergleich ist in der folgenden Grafik (Abb. 2) für ein festes Epsilon $\epsilon = 10^{-8}$ (Abbruchbedingung 1: eine ausreichend genaue Lösung wurde gefunden) dargestellt. \\
Es lässt sich beobachten, dass das direkte und das iterative Verfahren für den ein-, den zwei- und den dreidimensionalen Fall das exakt selbe Konvergenzverhalten aufweisen. 
Der Gauß-Algorithmus mit LU-Zerlegung und das CG-Verfahren stellen lediglich Lösungsverfahren zur Lösung eines linearen Gleichungssystems bereit. 
Die Methode zur Lösung des Poisson-Problems aber basiert in beiden Fällen auf einer Diskretisierung des Gebietes und des Laplace-Operators. 
Da die Konvergenzgeschwindigkeit sowohl beim LU- als auch beim CG-Verfahren abhängig von dieser Diskretisierung ist, ist eine gleiche Konvergenzgeschwindigkeit auch nicht verwunderlich. 
Noch immer gilt die Faustregel: Je feiner wir das Gitter wählen, desto genauer ist unsere numerische Lösung. 
Aufgrund der zweiten finiten Differenz liegt im eindimensionalen Fall quadratische Konvergenz, im zweidimensionalen Fall lineare und im dreidimensionalen Fall sublineare Konvergenz vor. \\


{
  \centering
    \includegraphics[width=0.75\textwidth]{Grafiken/compare}
    \vspace{-0.2cm}
    \captionof{figure}{Vergleich Konvergenzverhalten LU und CG mit $\epsilon = 10^{-8}$ für $d\in\{1, 2, 3\}$}
}
\vspace{0.5cm}


\subsection{Konvergenzverhalten für variable Epsilon}
Im Folgenden wollen wir das Konvergenzverhalten des CG-Verfahrens für eine Variation von Epsilon-Werten, die von der Feinheit der Diskretisierung abhängig ist, untersuchen. 
Es gilt $\epsilon^{(k)}=n^{-k}$, wobei $n$ wie bisher die Anzahl der Diskretisierungsintervalle in einer Dimension $d\in\{1, 2, 3\}$ angibt. \\
In der folgenden Abbildung 3 haben wir das Konvergenzverhalten für die drei untersuchten Dimensionen und die Variation der Epsilon-Werte grafisch dargestellt. 
Grundsätzlich sieht man, dass sich der Anstieg der Fehlerkurven mit zunehmendem $k$ immer weiter verringert. 
Für $k=-2$ und $k=0$ ist $\epsilon^{(k)}=1$. 
Dies bedeutet, dass das Verfahren bereits nach sehr wenigen Schritten abbricht, weil die Norm des Residuums schnell dieser Bedingung genügt. 
Wir erhalten in diesen Fällen eine relativ grobe Approximation der exakten Lösung, die sich mit einer feineren Diskretisierung nicht verbessern lässt, sondern langsam immer schlechter wird (für die drei betrachteten Dimensionen mit etwa gleich großem Anstieg). 
Diese Verschlechterung ist vermutlich auf die zunehmenden Rundungsfehler zurückzuführen. \\
Für $k>0$ wird mit immer größerem $n$ bzw. $N$, d.h. mit einer immer feineren Diskretisierung hingegen eine Straffung der Abbruchbedingung bewirkt. 
(Der Grad der Straffung hängt dabei von der Größe von $k$ ab.) 
Daher sind die zugehörigen Graphen monoton fallend. 
Für alle drei Dimensionen gilt, dass im Minimum für $k\geq4$ in etwa dasselbe Konvergenzverhalten wie für $\epsilon = 10^{-8}$ zu beobachten ist. 
Daraus lässt sich schließen, dass $\epsilon = 10^{-8}$ eine durchaus geeignete Wahl für die Abbruchbedingung (Obergrenze Norm des Residuums) darstellt. 
Für kleinere Dimensionen genügen u.U. allerdings bereits kleinere Werte für Epsilon. 
Die Festlegung eines passenden Wertes ist dabei immer auch abhängig von der Größe des linearen Gleichungssystems, welche wiederum von der Dimension und der Feinheit der Diskretisierung bedingt wird. \\


{
  \centering
    \includegraphics[width=0.45\textwidth]{Grafiken/epsilon_d1}
    \includegraphics[width=0.45\textwidth]{Grafiken/epsilon_d2}
    \includegraphics[width=0.45\textwidth]{Grafiken/epsilon_d3}
    \vspace{-0.2cm}
    \captionof{figure}{Konvergenzverhalten mit $\epsilon^{(k)}=n^{-k}$ für $d\in\{1, 2, 3\}$}
}
\vspace{0.5cm}


\pagebreak
\section{Abschließende Worte}
Gegenüberstellung direkt vs. iterativ
\\
sehr groß Gleichungssysteme mit dünn besetzter Matrix: Erzeugen neuer Nicht-Null-Elemente verursacht hohen Aufwand bei direkten Verfahren \\
hingegen arbeiten iterative Verfahren nach Algorithmen, die die Matrix nicht verändern, sind also deutlich anspruchsloser hinsichtlich des Speicherbedarfs \\
\\
Lösung des Poisson-Problem mittels Finite-Differenzen-Diskretisierung produziert dünn besetzte Blockmatrizen hoher Dimensionen > sinnvoll, iterative Lösungsverfahren zu nutzen \\
Gleichungssystemen mit mehreren rechten Seiten Zusatzaufwand bei direkten Verfahren gering \\
Aufwand bei iterativen Verfahren für jede neue rechte Seite oben drauf \\
\\
mehr Rechenoperationen bedeuten wiederum auch mehr Möglichkeiten, Fehler zu produzieren bzw. sich fortpflanzen zu lassen, insbesondere bei direkten Verfahren (ständiges Produzieren von Zwischenergebnissen und Weiterrechnen damit, Bsp. Vorwärts-/Rückwärtselimination) \\
iterative Verfahren fangen diese Fehler mehr oder minder auf, indem sie im schlimmsten Fall lediglich mehr Zeit benötigen, um eine ausreichend genaue Näherungslösung bereitstellen zu können \\
\\
CG-Verfahren: \\
gut geeignet für dünn besetzte Matrizen (z.B. hier betrachtete Poisson-Problem), Aufwand besteht hauptsächlich nur aus Matrix-Vektor-Multiplikation, für sehr große Systeme erhalten wir bereits in wenigen Schritten eine genügend genaue Approximation der exakten Lösung \\


\pagebreak
\bibliographystyle{plain}
\bibliography{serie5_literatur}

\end{document}
