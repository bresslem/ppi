\documentclass{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{framed}
\usepackage{xcolor}
\usepackage[nottoc]{tocbibind}
\usepackage{caption}
\usepackage{setspace}
\onehalfspacing

\colorlet{shadecolor}{gray!25}
\setlength{\parindent}{0pt}

\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\R}{\mathbb{R}}

\begin{document}

\title{Lösen des Poisson-Problems mittels Finite-Differenzen-Diskretisierung\\
und LU-Zerlegung}
\author{Marisa Breßler und Anne Jeschke (PPI27)}
\date{03.01.2020}
\maketitle

\tableofcontents

\pagebreak
\section{Einleitende Worte}
In unserem Bericht  vom 29.11.2019 haben wir das Poisson-Problem vorgestellt und einen numerischen Lösungsansatz aufgezeigt, der es mittels einer Diskretisierung des Gebietes und des Laplace-Operators in das Lösen eines linearen Gleichungssystems überführt.
Letzteres soll nun wie angekündigt durchgeführt werden.
In dieser Arbeit wollen wir das lineare Gleichungssystem direkt lösen.
Dazu nutzen wir die LU-Zerlegung (mit Spalten- und Zeilenpivotisierung) der ermittelten tridiagonalen Block-Matrix $A^d$.

Anhand einer Beispielfunktion und den bereits im vorherigen Bericht betrachteten Fällen des Einheitsintervalls, -quadrates, -würfels (d.h. für das Gebiet $\Omega\subset\R^d$ ($d\in\mathbb{N}$) und dessen Rand $\partial\Omega$ gilt:
$\Omega=(0,1)^d$, $d\in\{1, 2, 3\}$ mit der Randbedingung $u \equiv 0$ auf $\partial\Omega$, wobei $u$ die gesuchte Funktion ist)
wollen wir im Folgenden die Funktionalität (Genauigkeit/Fehler, Konvergenzgeschwindigkeit, Effizienz) dieses Lösungsverfahrens exemplarisch untersuchen.
Alle im Rahmen dessen nötigen theoretischen Grundlagen finden sich in unseren vorherigen Berichten.


\section{Untersuchungen zur Genauigkeit}
Für unsere Untersuchungen wählen wir die Beispielfunktion $u: \Omega \rightarrow \mathbb{R}$, die wie folgt definiert ist:
\[u(x) := \prod \limits_{l=1}^{d} x_l \, sin(\pi x_l)\]
Dabei sei wie bereits erwähnt $\Omega = (0,1)^d$ und $d\in\{1, 2, 3\}$.
Die Funktion $u$ ist die exakte Lösung des Poisson-Problems, sie wird in der Praxis gesucht. Bekannt ist lediglich die Funktion $f\in C(\Omega ; \R)$ und $\forall \, x \in\Omega$ gelte $-\Delta u(x) = f(x)$. Dementsprechend ist die Funktion $f: \Omega \rightarrow \mathbb{R}$ gegeben durch:
\[f(x) = -\pi \sum_{l=1}^d \left((2\,cos(\pi x_l) - \pi x_l sin(\pi x_l)) \prod_{i \in \{1,...,d\}\setminus\{k\}} x_i \, sin(\pi x_i) \right) \]

Die Genauigkeit unserer numerischen Lösung des Poisson-Problems -- wir nennen diese gesuchte Funktion $\hat{u}$ (denn sie ist die Approximation der exakten Lösungsfunktion $u$) -- ist abhängig von der Größenordnung der Fehler. Der Gesamtfehler setzt sich aus Verfahrens-/Approximationsfehler auf der einen und Rundungsfehler auf der anderen Seite zusammen\cite{tischendorf2019}. Im Folgenden wollen wir beide Fehlerarten in Hinblick auf unser Beispiel betrachten.

\subsection{Verfahrens-/Approximationsfehler}
Die Genauigkeit der berechneten numerischen Approximation wird höher,je mehr Diskretisierungspunkte man wählt, d.h. je größer die Anzahl der Intervalle $n$, bzw. je kleiner die Intervalllänge $h$ ist. Es gilt $h=n^{-1}$.

Dies wird im Folgenden beispielhaft für den Fall $d=2$ dargestellt.

{
  \centering
    \includegraphics[width=0.75\textwidth]{Grafiken/3D_n=5}
    \vspace{-0.2cm}
    \captionof{figure}{Approximierte Lösung, exakte Lösung und deren absolute Differenz für $n = 5$}
    \includegraphics[width=0.75\textwidth]{Grafiken/3D_n=10}
    \vspace{-0.2cm}
    \captionof{figure}{Approximierte Lösung, exakte Lösung und deren absolute Differenz für $n = 10$}
    \includegraphics[width=0.75\textwidth]{Grafiken/3D_n=20}
    \vspace{-0.2cm}
    \captionof{figure}{Approximierte Lösung, exakte Lösung und deren absolute Differenz für $n = 20$}
}
\vspace{0.5cm}

Schon bei der sehr groben Diskretisierung mit $n=5$ kann man den Unterschied zwischen den beiden Lösungen mit bloßem Auge kaum erkennen, weshalb wir uns entschieden haben auch die absolute Differenz der beiden darzustellen.
Wie man dort sehen kann, erreicht man durch Erhöhung der Anzahl der Intervalle eine immer genauere Approximation der exakten Lösungsfunktion. Die absolute Differenz der Approximation und der exakten Lösung hält sich in einer immer kleineren Größenordnung auf.

{
  \centering
    \includegraphics[width=0.75\textwidth]{Grafiken/loglogerr_d123_neu}
    \vspace{-0.2cm}
    \captionof{figure}{Konvergenzplot der maximalen Fehler in Abhängigkeit von $N$}
}
\vspace{0.5cm}

Man kann erkennen, dass mit größerem $N$, d.h. mit mehr Diskretisierugnspunkten, der Fehler immer kleiner wird, wobei er sich bei höheren Dimensionen langsamer verkleinert.
Für $d=1$ lässt sich in der Abbildung eine Konvergenzgeschwindigkeit von $N^{-2}$ erkennen. Für $d=2$ hingegen nur noch eine von $N^{-1}$ und bei $d=3$ von $N^{-1/2}$.


\subsection{Rundungsfehler}
Für ein mathematisches Problem gibt seine Kondition den Faktor an, um den sich ein Fehler in den Eingangsdaten maximal auf die Lösung auswirken kann.
Die Kondition eines linearen Gleichungssystems $Ax = b$ mit regulärem $A\in\mathbb{R}$ ist gegeben durch \[cond(A) = ||A||\cdot||A^{-1}||\]

Die folgende Grafik zeigt die Entwicklung der Kondition der Matrix $A^{(d)}\in\mathbb{R}^{N \times N}$ in Abhängigkeit von $N$.

{
  \centering
    \includegraphics[width=0.75\textwidth]{Grafiken/loglogcond_d123_neu}
    \vspace{-0.2cm}
    \captionof{figure}{Kondition der Matrix in Abhängigkeit von $N$}
}
\vspace{0.5cm}

Man kann beobachten, dass die Kondition mit der gleichen Geschwindigkeit steigt, mit der für das jeweilige $d \in \{ 1, 2, 3\}$ der absolute Fehler sinkt.
(BEGRÜNDUNG?)

Vergleicht man jedoch, wie in den Tabellen 1 bis 3 dargestelllt, die Kondition der Matrix $A^{(d)}$ mit der der Hilbertmatrix $H_N$ der gleichen Dimension, definiert durch

\[H_N =
\begin{pmatrix}
  1 & \frac{1}{2} & \frac{1}{3} & \cdots & \frac{1}{N} \\
  \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \cdots & \frac{1}{N+1} \\
  \frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \cdots & \frac{1}{N+2} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  \frac{1}{N} & \frac{1}{N+1} & \frac{1}{N+2} & \cdots & \frac{1}{2N-1}
\end{pmatrix}\]

kann man erkennen, dass die Kodition der Matrix $A^{(d)}$ im Vergleich zur Hilbertmatrix nur ein sehr moderates Wachstum besitzt.

\begin{table}[ht!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{N} & $\mathbf{cond(A^{(d)})}$ & $\mathbf{cond(H_N)}$   \\ \hline
1          & 1.0                       & 1.0                \\ \hline
2          & 3.0                       & 27.00000000000001  \\ \hline
3          & 7.999999999999998         & 748.0000000000027  \\ \hline
4          & 11.999999999999998        & 28374.999999997388 \\ \hline
5          & 17.999999999999996        & 943655.9999999335  \\ \hline
6          & 24.0                      & 29070279.002940644 \\ \hline
7          & 32.0                      & 985194889.719848   \\ \hline
8          & 40.00000000000001         & 33872790819.49471  \\ \hline
9          & 50.0                      & 1099650991701.052  \\ \hline
\end{tabular}
\caption{Vergleich der Kondition von $A^{(1)}$ und der entsprechenden Hilbertmatrix}
\end{table}

\begin{table}[ht!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{N} & $\mathbf{cond(A^{(d)})}$  & $\mathbf{cond(H_N)}$       \\ \hline
1          & 1.0                       & 1.0                    \\ \hline
4          & 2.9999999999999996        & 28374.999999997388     \\ \hline
9          & 9.000000000000002         & 1099650991701.052      \\ \hline
16         & 13.333333333333334        & 2.311624234251581e+18  \\ \hline
25         & 20.769230769230763        & 1.7424649140711887e+19 \\ \hline
36         & 27.448275862068964        & 1.380385024104188e+19  \\ \hline
49         & 37.264705882352935        & 3.780222548906899e+19  \\ \hline
64         & 46.29522752497227         & 1.5982160238410445e+19 \\ \hline
81         & 58.47874842732894         & 8.024854808108397e+19  \\ \hline
\end{tabular}
\caption{Vergleich der Kondition von $A^{(2)}$ und der entsprechenden Hilbertmatrix}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{N} & $\mathbf{cond(A^{(d)})}$ & $\mathbf{cond(H_N)}$       \\ \hline
1          & 1.0                       & 1.0                    \\ \hline
8          & 3.0000000000000004        & 33872790819.49471      \\ \hline
27         & 9.882352941176464         & 1.6381885442111791e+19 \\ \hline
64         & 14.526315789473696        & 1.5982160238410445e+19 \\ \hline
125        & 23.31313131313129         & 5.942029981401014e+19  \\ \hline
216        & 30.627549372748764        & 9.99739805300818e+20   \\ \hline
343        & 42.176769881272705        & 9.981011731951971e+20  \\ \hline
512        & 52.174333259988614        & 2.72516709557988e+22   \\ \hline
729        & 66.44907705653814         & 4.0429517177716715e+21 \\ \hline
\end{tabular}
\caption{Vergleich der Kondition von $A^{(3)}$ und der entsprechenden Hilbertmatrix}
\end{table}

\pagebreak
\section{Untersuchungen zum Speicherplatz}
Für die Durchführung der Experimente haben wir die Matrizen wie schon im vorherigen Bericht beschrieben im sparse-Format gespeichert.
Für die Matrizen $A^{(d)}$ hatten wir dort bereits ermittelt, dass sich dies schon für niedrige $n$ lohnt, da das sparse-Format sich nur die nicht-Null-Einträge merkt.
Da für jeden nicht-Null-Eintrag drei Werte gespeichert werden müssen, lohnt es sich das sparse-Format zu nutzen, sobald weniger als ein Drittel der Einträge einer Matrix ungleich Null sind.
In der unteren Grafik sieht man die Anzahl der nicht-Null-Einträge von $A^{(d)}$ und der zugehörigen LU-Zerlegung für $d\in\{1, 2, 3\}$ in Abhängigkeit von $N$.
Desweiteren haben wir die eine Linie eingezeichnet, die ein Drittel der Matrix Einträge repräsentiert.

{
  \centering
    \includegraphics[width=0.75\textwidth]{Grafiken/non-zero_d1}
    \includegraphics[width=0.75\textwidth]{Grafiken/non-zero_d2}
    \includegraphics[width=0.75\textwidth]{Grafiken/non-zero_d3}
    \vspace{-0.2cm}
    \captionof{figure}{Anzahl der nicht-Null-Einträge von $A^{(d)}$ und der LU-Zerlegung für $d\in\{1, 2, 3\}$}
}
\vspace{0.5cm}

Für $d=1$ kann man beobachten, dass nach $N=9$ bzw. $n=10$  weniger ein Drittel der Matrixeinträge von sowohl $A^{(d)}$ als auch der LU-Zerlegung ungleich Null sind.
Für $d=2$ gilt dies schon ab $N=25$ bzw. $n=6$ und für $d=3$ ab $N = 125$ bzw. auch für $n=6$.
Auch für die LU-Zerlegung lohnt es sich also schon für sehr niedrige $n$ die Matrix im sparse-Format zu speichern.

\pagebreak
\section{Zusammenfassung und Ausblick}


\pagebreak

\bibliographystyle{plain}
\bibliography{serie3_literatur}

\end{document}